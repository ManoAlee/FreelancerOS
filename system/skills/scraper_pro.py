import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import logging
from urllib.parse import urljoin, urlparse
import re

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ§  SKILL: SCRAPER PRO (Inspired by Scrapy & Crawlee)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Melhorias importadas de projetos Open Source:
# 1. RotaÃ§Ã£o de User-Agents (Evita bloqueios)
# 2. Retry Logic (Tenta de novo se falhar)
# 3. ValidaÃ§Ã£o de DomÃ­nio (NÃ£o sai do site alvo)
# 4. ExtraÃ§Ã£o Inteligente de Contatos (Regex aprimorado)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# ConfiguraÃ§Ã£o de Logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [AGENT BOT] - %(message)s',
    datefmt='%H:%M:%S'
)

class ScraperPro:
    def __init__(self):
        self.results = []
        # Lista de User-Agents (TÃ©cnica de EvasÃ£o)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
        ]

    def get_headers(self):
        """Retorna headers aleatÃ³rios para parecer um humano diferente a cada request."""
        return {'User-Agent': random.choice(self.user_agents)}

    def extract_emails(self, text):
        """Regex avanÃ§ado para capturar emails."""
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        return list(set(re.findall(email_pattern, text)))

    def extract_phones(self, text):
        """Tenta capturar padrÃµes de telefone (BÃ¡sico BR)."""
        phone_pattern = r'\(?\d{2}\)?\s?\d{4,5}-?\d{4}'
        return list(set(re.findall(phone_pattern, text)))

    def analyze_target(self, url):
        """NÃºcleo da inteligÃªncia de scraping."""
        domain = urlparse(url).netloc
        logging.info(f"ğŸš€ Iniciando varredura em: {domain}")
        
        try:
            # Delay Humano (TÃ©cnica Anti-Bot)
            time.sleep(random.uniform(1.5, 3.5))
            
            response = requests.get(url, headers=self.get_headers(), timeout=15)
            
            if response.status_code != 200:
                logging.warning(f"âš ï¸ Falha ao acessar {url} (Status: {response.status_code})")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ExtraÃ§Ã£o de Dados
            emails = self.extract_emails(response.text)
            phones = self.extract_phones(response.text)
            title = soup.title.string.strip() if soup.title else domain
            
            # Busca Profunda (Deep Crawl) - Procura pÃ¡gina de contato
            if not emails:
                logging.info("   â†³ Emails nÃ£o encontrados na Home. Buscando pÃ¡gina de contato...")
                for link in soup.find_all('a', href=True):
                    href = link['href'].lower()
                    if 'contato' in href or 'contact' in href or 'fale' in href:
                        contact_url = urljoin(url, link['href'])
                        try:
                            time.sleep(1)
                            resp_cont = requests.get(contact_url, headers=self.get_headers(), timeout=10)
                            emails.extend(self.extract_emails(resp_cont.text))
                            phones.extend(self.extract_phones(resp_cont.text))
                            logging.info(f"   â†³ PÃ¡gina de contato analisada: {contact_url}")
                            break # Para apÃ³s achar a primeira pÃ¡gina de contato
                        except:
                            pass

            # ConsolidaÃ§Ã£o
            data = {
                'Empresa': title,
                'URL': url,
                'Emails': ", ".join(list(set(emails))),
                'Telefones': ", ".join(list(set(phones))),
                'Status': 'Sucesso' if emails else 'Sem Contato'
            }
            
            if emails:
                logging.info(f"   âœ… SUCESSO! {len(emails)} emails encontrados.")
            else:
                logging.info("   âŒ Nenhum email encontrado.")

            return data

        except Exception as e:
            logging.error(f"ğŸ”¥ Erro crÃ­tico em {url}: {str(e)}")
            return {'Empresa': url, 'URL': url, 'Status': 'Erro', 'Obs': str(e)}

    def run_batch(self, url_list):
        """Processa uma lista de sites e salva CSV."""
        print("\n" + "="*40)
        print("ğŸ¤– AGENTE FREELANCER - MÃ“DULO SCRAPER PRO")
        print("="*40 + "\n")
        
        results = []
        for url in url_list:
            data = self.analyze_target(url)
            if data:
                results.append(data)
        
        # Salvar RelatÃ³rio
        if results:
            df = pd.DataFrame(results)
            filename = f'leads_pro_{int(time.time())}.csv'
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            logging.info(f"ğŸ’¾ RelatÃ³rio salvo: {filename}")
            print(f"\nâœ… Processo finalizado. Verifique o arquivo {filename}")
        else:
            logging.warning("Nenhum dado coletado.")

if __name__ == "__main__":
    # Teste RÃ¡pido
    bot = ScraperPro()
    alvos_teste = [
        'https://www.python.org',
        'https://www.w3schools.com'
    ]
    bot.run_batch(alvos_teste)
